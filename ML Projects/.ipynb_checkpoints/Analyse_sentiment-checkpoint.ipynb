{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de sentiment avec Keras\n",
    "\n",
    "Pour réaliser ce projet, nous allons utiliser le data set IMDB, disponible directement dans `keras.datasets`. Il contient 25000 critiques de films. Nous classifirons les critiques en deux catégories, positive et négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des données\n",
    "\n",
    "Nous commençons par extraire les données du data set dans notre environnement en deux parties:\n",
    "- La première contient les données d'entrainement (train set)\n",
    "- La seconde contient les données d'evaluation (test set)\n",
    " \n",
    "Nous limitons le nombre de mots à 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_MOTS = 5000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_NUM_MOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque critique a un nombre de mots différente, nous devons alors fixer une un nombre de mots maximum pour chaque critique. Après cette case, toutes les critiques feront la même taille. Prenons deux exemples de critique pour illustrer mon propos.\n",
    "\n",
    "Si nous avons la critique, \"très bon film\" dans nos données, `sequence.pad_sequence()` l'a convertira en ceci \"très bon film 0 0 0 0 0 0 0 0... x497\".\n",
    "\n",
    "Et enfin, si nous avons une critique qui contient 560 mots alors `sequence.pad_sequence()` enlevera les 60 derniers mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TAILLE_CRITIQUE = 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_TAILLE_CRITIQUE)\n",
    "X_test  = sequence.pad_sequences(X_test, maxlen=MAX_TAILLE_CRITIQUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle\n",
    "\n",
    "Nous allons désormais créer notre modèle, et nous ajoutons nos couches: \n",
    "\n",
    "- Une couche `Embedding`, avec pour dimension d'entrée (`input_dim`) le nombre maximum de mots (ici, 500), puis la dimension de sortie (`output_dim`) est de 32, donc la couche retournera pour chaque mot, une liste de 32 nombres qui auront une relation mathématique entre eux.\n",
    "- Une couche `LSTM` avec une dimension de sortie de 100 unités.\n",
    "- Pour finir, une couche `Dense` avec une seule unité ainsi qu'une fonction d'activation `sigmoid` qui ensuite contiendra la prédiction de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TAILLE_VECTEUR = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_NUM_MOTS, output_dim=EMBEDDING_TAILLE_VECTEUR, input_length=MAX_TAILLE_CRITIQUE))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous compilons notre modèle avec pour fonction de coût `binary crossentropy`, cette fonction nous aides à décider si le résultat devrait être 0 ou 1. Pour optimiser notre modèle, nous utiliserons `adam` et enfin, nous l'évaluerons grâce à `accuracy` (précision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé du modèle\n",
    "\n",
    "Pour la couche `Embedding`, chacun de nos mots est converti en liste de 32 nombres, nous avons 5000 mots (32 x 5000 = 160000).\n",
    "\n",
    "Puis, la couche `LSTM` réduit le nombre de paramètres à 5320 (5000 x 100 + 32 x 100 = 53200).\n",
    "\n",
    "Et enfin, la couche `Dense` est connecté aux 100 unités de la couche précédente (100 + 1 = 101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainer le modèle\n",
    "\n",
    "Nous allons entrainer notre modèle sur les données d'entrainement (train set) sur trois cycles (`epochs=3`) par groupe de 64 (`batch_size=64`). La précision de notre modèle devrait s'améliorer après chaque cycle. Cette étape devrait prendre ~10 minutes selon votre ordinateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\redad\\miniconda3\\envs\\artificial_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 528s 21ms/step - loss: 0.4709 - accuracy: 0.7706\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 588s 24ms/step - loss: 0.3505 - accuracy: 0.8574\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 646s 26ms/step - loss: 0.2560 - accuracy: 0.9005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25385574cf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation de notre modèle\n",
    "\n",
    "Notre modèle à une precision de 90% sur nos données d'entrainement. Nous allons désormais l'évaluer sur son apprentissage en lui donnant des données qu'il n'a jamais vu avant, les données d'évaluation (test set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle a une précision de 86.94 %\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Notre modèle a une précision de\", round(scores[1]*100, 2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Notre modèle finit avec ~87% de précision sur les données d'évaluation.\n",
    "\n",
    "Cela signifie que notre modèle peut prédire avec ~87% de précision si la critique d'un film est positive ou négative.\n",
    "\n",
    "Notre ordinateur a classé 25000 critiques de films avec un bon résultat en seulement 10 minutes.\n",
    "\n",
    "Pour améliorer notre modèle, nous pourrions améliorer la vitesse d'entrainement(utilisez un GPU, par exemple) ainsi que son taux de précision (compléxifiez notre modèle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
